\documentclass[advisory-ia.tex]{subfiles}

\DoIfAndOnlyIfStandAlone{%
  \externaldocument{Crit_A_Planning.tex}%
  \externaldocument{Crit_B_Design.tex}%
  \externaldocument{Crit_E_Evaluation.tex}%
  \externaldocument{appendix.tex}%
}%

\begin{document}
  \section{Development}
  \label{sec:develop}
  After meeting with my client, I felt it most appropriate for the problem at hand to create a multi-layered application hosted on the cloud.
  With my multi-layered architecture, the layers were established as the Frontend, Backend, and Database layers.
  The Frontend is a static website with its assets hosted in an AWS S3 bucket.
  The Backend is a webserver with a RESTful API implemented in Rust, containerized using Docker, and hosted on AWS Elastic Container Service deployed using AWS Fargate.
  The Database is the Neo4j Graph Database, using an official docker image and similarly deployed to AWS ECS in order to communicate well with the Backend.
  The development details for the Frontend and Backend are detailed in \cref{subsec:Frontend,subsec:Backend} respectively.

  \subsection{Frontend}
  \label{subsec:Frontend}
  Starting this project, the part I was most weary of was the Frontend.
  I had created several Backends before, but had never ventured into CSS or Frontend frameworks.
  At my summer internship, some of my fellow interns developed the Frontend using React, which is considered the industry standard, but I wanted to try a developer-friendly framework I had heard about called Svelte.
  My biggest challenges faced in this area were: working with CSS (especially making my website work on varying screen sizes) and the somewhat-limited Svelte ecosystem.
  I wanted to use Google's Material UI (MUI) for on-screen assets such as buttons, text boxes, etc., but found that the Svelte implementation of the standards, called SMUI, was poorly documented, rapidly evolving, and did not implement the MUI specifications completely.

  As currently implemented, my Frontend is comprised of five pages, most of them for authentication purposes.
  I implemented authentication and authorization using AWS Cognito User Pools.
  A request is sent to AWS Cognito using a Javascript library, and the Frontend is returned a JSON web token that can be included in the Authentication header in future requests to the Backend.
  As is, tokens are not stored in a cookie, though they persist between different pages of the site.
  Using a cookie for the auth token is complicated because the token expires after a while, and the site would need to distinguish between a valid and expired token before refreshing.
  This is an avenue for future improvement.

  In order to allow my client to quickly add students and their relationships with their teachers, my product allows importing from a .xlsx spreadsheet.
  This spreadsheet is created by school administration every year, and only needs the tweak of adding student sex in order to use.
  The algorithm to extract data from the spreadsheet (once transformed to a 2d array) is shown in \cref{lst:import_students,lst:import_teachers}.
  \cref{lst:import_students} (importing students) is significantly more complicated than \cref{lst:import_teachers} (importing teachers), as it needs to extract the student's teachers over multiple rows.
  \cref{lst:import_students,lst:import_teachers} both use the Set datastructure in order to forbid duplicates.

  \subfile{code/student_import}

  \subfile{code/teacher_import}

  The functional portion of the Frontend is a dashboard that allows for the user to tweak certain settings for the advisory generation program.
  These settings are: number of generated advisories, and the relative weights of different factors in the generation process such as whether or not the advisory contains one of the student's teachers, grade diversity, and sex diversity.
  These are sent to the Backend when the HTTP request to build the advisories is made.
  These could be stored as a cookie or as a user-specific setting, but, similarly to the auth token, this is an avenue for future improvement to the product.

  \subsection{Backend}
  \label{subsec:Backend}
  The Backend is significantly more complicated than the Frontend, though it was easier for me to build.

  The Backend consists of an HTTP server that forwards requests to certain handlers based on path.
  An example of a handler is \cref{lst:get_people_handler}.

  \subfile{code/get_people_handler}

  The handlers call methods of several different structs such as Student, Teacher, Person, Advisory, and Organization.
  These methods handle the actual business logic of placing data from the Frontend in the database, retrieving it, generating advisories, etc.

  \subsubsection{Core Logic}
  The core logic for the product generates a group of advisories (referred to within the code as an \enquote{Organization}) using data from the database.
  The method relies on several other functions and methods in order to work, but the gist of its functionality is apparent in \cref{lst:generate}.
  The method calculates a weighted value taking into account the relative weights set by the user on the Frontend between each Student and Advisory.
  The algorithm then adds the student to the Advisory with the highest weighted value.
  For the diversity weights (grade and sex), each advisory has a quota for each possible sex and grade.
  The algorithm takes this into account and decreases or increases the weighted value in accordance with the remaining values for the quota.
  Should the student be added to the some advisory, then the remaining quotas for their respective sex and grade are decreased by one.

  Pairs of students can be prevented from being placed in the same advisory through a specific path.
  This creates a relationship in the graph database that is later stored in a vector on a per-student basis of all students that they should not be paired with.
  When each student's weight for an advisory is being calculated (\cref{lst:calculate_weight}), a method checks whether the advisory already contains one of the people that the student is banned from being placed with.
  Should that be the case, the weighted value is reduced by 10,000, in order to effectively eliminate the chance of the student being placed in said advisory.

  \subfile{code/calculate_weight}

  \subfile{code/generate}

  \subsubsection{Refactor}
  I built the Backend twice, once just to get it working, then I refactored it to improve performance and readability.
  In my refactor I made usage of more polymorphism through the use of \emph{traits}, a feature of Rust similar to interfaces in languages like Java.
  Using this simplified my code and allowed for more structure.
  \cref{lst:old_add_teacher} shows the pre-refactor code to add a teacher to the database.
  Each struct that could be added to the database had an almost identical function, so I created a trait and simplified it to \cref{lst:new_add_teacher}.
  \cref{lst:new_add_teacher} uses generics and is run as a method of Teacher using dot notation, versus in \cref{lst:old_add_teacher} where a Teacher object must be passed as a parameter.

  \subfile{code/old_add_teacher}

  \subfile{code/new_add_teacher}

  Similar changes were made for many different functions, especially for those that interact with the database.

  One of the biggest changes made in my refactor was applied to my bulk database queries.
  Originally, I had sent a separate query for each Student/Teacher, but during my refactor I figured out a way to offload this loop to be executed by the database, cutting down the number of slow transactions.

  \subsubsection{Error Handling}
  One thing that I tried to emphasize both originally and in my refactor was error handling.
  One of my favorite features of the Rust programming language is the Result generic type.
  This type represents both a successful return type and an error type in the format \texttt{Result<T, E>}.
  This type allows functions to defer error handling to the caller, which avoids patterns such as \texttt{try...catch} common in other languages.
  Since error types are Enums, they are handled idiomatically through the \texttt{match} syntax \cref{lst:match_syntax}.

  \subfile{code/match_syntax}

  \subsection{Use of Libraries \& Frameworks}
  Both my Frontend and Backend make extensive usage of third party software and existing code to function.
  Both the Rust and JavaScript ecosystems make heavy use of small packages both official and otherwise.
  A list of these can be found in the \texttt{Cargo.toml} and \texttt{package.json} files in the Frontend and Backend respectively.
\end{document}
